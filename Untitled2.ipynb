{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heechan3006/CS/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjA07RpClSBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAux2SB5lVzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2fd580f1-fdf6-4230-9080-70f337841cba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS27PthbmzL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a94e1197-aded-49c9-e213-f499f04cc04b"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClvIbRuSmVrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "path = \"drive/My Drive\"\n",
        "\n",
        "with open(os.path.join(path,'train_vision.csv'),'r') as f:\n",
        "  c = csv.reader(f)\n",
        "  files = [i for i in c]\n",
        "files = files[1:]\n",
        "files = [[os.path.join(path+'/faces_images/',i[0]),i[1]] for i in files]\n",
        "random.shuffle(files)\n",
        "\n",
        "TRAIN_RATIO = 0.8\n",
        "NUM_IMAGES = len(files)\n",
        "train_files = files[:(int)(NUM_IMAGES*TRAIN_RATIO)]\n",
        "val_files = files[(int)(NUM_IMAGES*TRAIN_RATIO):]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SRtD0ibt-9z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2ba5411-3c14-4bf1-a714-6760f6af3e2f"
      },
      "source": [
        "files[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['drive/My Drive/face_5747.png', '4']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hYNH55LoxBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "NUM_CLASSES= 6\n",
        "def ImageGenerator(images_path,batch_size, target_size,preprocess_input,shuffle=True):\n",
        "  if shuffle==True:\n",
        "    np.random.shuffle(images_path)\n",
        "  zipped = itertools.cycle(images_path)\n",
        "  while True:\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(batch_size):\n",
        "      im,cls = next(zipped)\n",
        "      img = cv2.imread(im)\n",
        "      img = img[:,:,(2,1,0)]\n",
        "      img = preprocess_input(np.float32(img))\n",
        "      X.append(img)\n",
        "      Y.append(int(cls)-1)\n",
        "      yield np.array(X),to_categorical(Y,num_classes=NUM_CLASSES)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TICYTqgwoS9D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE=32\n",
        "TARGET_SIZE=128\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input,ResNet50V2\n",
        "from tensorflow.keras import models,layers,optimizers\n",
        "\n",
        "train_datagen = ImageGenerator(train_files,BATCH_SIZE,TARGET_SIZE,preprocess_input=preprocess_input)\n",
        "val_datagen = ImageGenerator(val_files,1,TARGET_SIZE,preprocess_input=preprocess_input,shuffle=False)\n",
        "\n",
        "train_len = len(train_files)\n",
        "val_len = len(val_files)\n",
        "steps_per_epochs_train = int(float(train_len) / float(BATCH_SIZE))\n",
        "steps_per_epochs_val = val_len\n",
        "model = ResNet50V2(weights='imagenet',include_top=False,input_shape=(TARGET_SIZE,TARGET_SIZE,3))\n",
        "'''for layer in model.layers[:-4]:\n",
        "  layer.trainable = False'''\n",
        "new_model = models.Sequential()\n",
        "new_model.add(model)\n",
        "new_model.add(layers.GlobalAvgPool2D())\n",
        "new_model.add(layers.Dropout(0.5))\n",
        "new_model.add(layers.Dense(NUM_CLASSES,activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdNN_LLZsZnL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "10fc93a1-5096-4c22-969c-3b53e72f3346"
      },
      "source": [
        "new_model.summary()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "resnet50v2 (Model)           (None, 4, 4, 2048)        23564800  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 6)                 12294     \n",
            "=================================================================\n",
            "Total params: 23,577,094\n",
            "Trainable params: 1,067,014\n",
            "Non-trainable params: 22,510,080\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4ih2QyvsuQ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6cd017d-ce0c-463d-cd96-685ffd5dcbbc"
      },
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 50\n",
        "MODEL_NAME='resnetv2'\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint,LearningRateScheduler,Callback\n",
        "import math\n",
        "\n",
        "save_path = os.path.join(path,'ckpt')\n",
        "def step_decay(epoch):\n",
        "  initial_learning_rate=LEARNING_RATE\n",
        "  drop=0.9\n",
        "  epochs_drop=10.0\n",
        "  lrate= initial_learning_rate*math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
        "  print('initial learning rate: {}, current learning rate: {}'.format(LEARNING_RATE,lrate))\n",
        "  return lrate\n",
        "\n",
        "def categorical_crossentropy(y_true,y_pred):\n",
        "  return K.categorical_crossentropy(y_true,y_pred)\n",
        "\n",
        "\n",
        "input_img = layers.Input(shape=(TARGET_SIZE,TARGET_SIZE,3))\n",
        "model_dir = os.path.join(save_path,MODEL_NAME)\n",
        "'''\n",
        "if tf.io.gfile.exists(model_dir):\n",
        "  tf.io.gfile.remove(model_dir)\n",
        "  print('file deleted')\n",
        "'''\n",
        "tf.io.gfile.makedirs(model_dir)\n",
        "\n",
        "adam = optimizers.Adam(lr=LEARNING_RATE,decay=1e-5)\n",
        "new_model.compile(optimizer=adam,\n",
        "                  loss=[categorical_crossentropy])\n",
        "best_checkpointer = ModelCheckpoint(os.path.join(save_path,MODEL_NAME+'/model_best_valloss{val_loss:.3f}.h5'),\n",
        "                                    monitor='val_loss',\n",
        "                                    priod=1,\n",
        "                                    verbose=1,\n",
        "                                    mode='min',\n",
        "                                    save_best_only=True,\n",
        "                                    save_weights_only=True)\n",
        "\n",
        "lrate = LearningRateScheduler(step_decay)\n",
        "callback_list = [lrate,best_checkpointer]\n",
        "hist = new_model.fit_generator(train_datagen,\n",
        "                               epochs=EPOCHS,\n",
        "                               steps_per_epoch=steps_per_epochs_train,\n",
        "                               validation_data=val_datagen,\n",
        "                               validation_steps=steps_per_epochs_val,\n",
        "                               workers=1,\n",
        "                               callbacks=callback_list)\n",
        "print('Validating ...')\n",
        "history = hist.history\n",
        "out_loss_curve = os.path.join(model_dir,'lr_curve.txt')\n",
        "with open(out_loss_curve,'w') as f:\n",
        "  f.write('train_cross_entropy_loss\\n')\n",
        "  for l in history['dense_1_loss']:\n",
        "    f.write(str(l))\n",
        "    f.write(' ')\n",
        "  f.write('\\n')\n",
        "  f.write('val_cross_entropy_loss\\n')\n",
        "  for l in history['val_dense_1_loss']:\n",
        "    f.write(str(l))\n",
        "    f.write(' ')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 1/50\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.4647\n",
            "Epoch 00001: val_loss improved from inf to 1.98062, saving model to drive/My Drive/ckpt/resnetv2/model_best_valloss1.981.h5\n",
            "146/146 [==============================] - 435s 3s/step - loss: 0.4611 - val_loss: 1.9806\n",
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 2/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2743\n",
            "Epoch 00002: val_loss improved from 1.98062 to 1.75074, saving model to drive/My Drive/ckpt/resnetv2/model_best_valloss1.751.h5\n",
            "146/146 [==============================] - 131s 894ms/step - loss: 0.2731 - val_loss: 1.7507\n",
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 3/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2607\n",
            "Epoch 00003: val_loss improved from 1.75074 to 1.47217, saving model to drive/My Drive/ckpt/resnetv2/model_best_valloss1.472.h5\n",
            "146/146 [==============================] - 131s 896ms/step - loss: 0.2592 - val_loss: 1.4722\n",
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 4/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2661\n",
            "Epoch 00004: val_loss improved from 1.47217 to 0.65762, saving model to drive/My Drive/ckpt/resnetv2/model_best_valloss0.658.h5\n",
            "146/146 [==============================] - 136s 930ms/step - loss: 0.2636 - val_loss: 0.6576\n",
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 5/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2664\n",
            "Epoch 00005: val_loss did not improve from 0.65762\n",
            "146/146 [==============================] - 132s 902ms/step - loss: 0.2637 - val_loss: 2.7253\n",
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 6/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2699\n",
            "Epoch 00006: val_loss did not improve from 0.65762\n",
            "146/146 [==============================] - 131s 897ms/step - loss: 0.2674 - val_loss: 1.7862\n",
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 7/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2505\n",
            "Epoch 00007: val_loss did not improve from 0.65762\n",
            "146/146 [==============================] - 127s 871ms/step - loss: 0.2480 - val_loss: 0.8735\n",
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 8/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2290\n",
            "Epoch 00008: val_loss did not improve from 0.65762\n",
            "146/146 [==============================] - 128s 877ms/step - loss: 0.2271 - val_loss: 0.8952\n",
            "initial learning rate: 0.0001, current learning rate: 0.0001\n",
            "Epoch 9/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2064\n",
            "Epoch 00009: val_loss did not improve from 0.65762\n",
            "146/146 [==============================] - 130s 892ms/step - loss: 0.2086 - val_loss: 0.9185\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 10/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2375\n",
            "Epoch 00010: val_loss did not improve from 0.65762\n",
            "146/146 [==============================] - 133s 911ms/step - loss: 0.2360 - val_loss: 0.8201\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 11/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2358\n",
            "Epoch 00011: val_loss did not improve from 0.65762\n",
            "146/146 [==============================] - 130s 888ms/step - loss: 0.2345 - val_loss: 0.8616\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 12/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2173\n",
            "Epoch 00012: val_loss did not improve from 0.65762\n",
            "146/146 [==============================] - 134s 921ms/step - loss: 0.2152 - val_loss: 0.9154\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 13/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2025\n",
            "Epoch 00013: val_loss improved from 0.65762 to 0.63256, saving model to drive/My Drive/ckpt/resnetv2/model_best_valloss0.633.h5\n",
            "146/146 [==============================] - 133s 909ms/step - loss: 0.2008 - val_loss: 0.6326\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 14/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1609\n",
            "Epoch 00014: val_loss improved from 0.63256 to 0.50530, saving model to drive/My Drive/ckpt/resnetv2/model_best_valloss0.505.h5\n",
            "146/146 [==============================] - 133s 908ms/step - loss: 0.1596 - val_loss: 0.5053\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 15/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1182\n",
            "Epoch 00015: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 134s 921ms/step - loss: 0.1179 - val_loss: 1.0218\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 16/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2139\n",
            "Epoch 00016: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 133s 910ms/step - loss: 0.2122 - val_loss: 1.9050\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 17/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2015\n",
            "Epoch 00017: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 130s 890ms/step - loss: 0.1997 - val_loss: 0.7598\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 18/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1474\n",
            "Epoch 00018: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 129s 881ms/step - loss: 0.1505 - val_loss: 0.7390\n",
            "initial learning rate: 0.0001, current learning rate: 9e-05\n",
            "Epoch 19/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1482\n",
            "Epoch 00019: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 131s 897ms/step - loss: 0.1470 - val_loss: 0.6634\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 20/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.3087\n",
            "Epoch 00020: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 127s 868ms/step - loss: 0.3056 - val_loss: 0.7546\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 21/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1906\n",
            "Epoch 00021: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 135s 921ms/step - loss: 0.1893 - val_loss: 0.5816\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 22/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2206\n",
            "Epoch 00022: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 129s 884ms/step - loss: 0.2188 - val_loss: 0.6381\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 23/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1538\n",
            "Epoch 00023: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 131s 894ms/step - loss: 0.1523 - val_loss: 0.7557\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 24/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1690\n",
            "Epoch 00024: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 131s 896ms/step - loss: 0.1676 - val_loss: 0.9437\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 25/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1350\n",
            "Epoch 00025: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 132s 907ms/step - loss: 0.1527 - val_loss: 0.7086\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 26/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1363\n",
            "Epoch 00026: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 133s 911ms/step - loss: 0.1351 - val_loss: 0.6393\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 27/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2884\n",
            "Epoch 00027: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 134s 919ms/step - loss: 0.2890 - val_loss: 0.5905\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 28/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1571\n",
            "Epoch 00028: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 129s 881ms/step - loss: 0.1559 - val_loss: 0.6186\n",
            "initial learning rate: 0.0001, current learning rate: 8.1e-05\n",
            "Epoch 29/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1487\n",
            "Epoch 00029: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 131s 894ms/step - loss: 0.1476 - val_loss: 0.5878\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 30/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.0941\n",
            "Epoch 00030: val_loss did not improve from 0.50530\n",
            "146/146 [==============================] - 135s 925ms/step - loss: 0.0940 - val_loss: 1.2886\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 31/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1033\n",
            "Epoch 00031: val_loss improved from 0.50530 to 0.45075, saving model to drive/My Drive/ckpt/resnetv2/model_best_valloss0.451.h5\n",
            "146/146 [==============================] - 135s 924ms/step - loss: 0.1040 - val_loss: 0.4508\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 32/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1659\n",
            "Epoch 00032: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 131s 898ms/step - loss: 0.1648 - val_loss: 0.5023\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 33/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1456\n",
            "Epoch 00033: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 120s 822ms/step - loss: 0.1446 - val_loss: 0.4890\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 34/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1412\n",
            "Epoch 00034: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 120s 821ms/step - loss: 0.1473 - val_loss: 0.5937\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 35/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1762\n",
            "Epoch 00035: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 119s 815ms/step - loss: 0.1747 - val_loss: 0.7942\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 36/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1740\n",
            "Epoch 00036: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 120s 819ms/step - loss: 0.1787 - val_loss: 0.8783\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 37/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.2132\n",
            "Epoch 00037: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 119s 812ms/step - loss: 0.2117 - val_loss: 0.6081\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 38/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1622\n",
            "Epoch 00038: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 118s 811ms/step - loss: 0.1610 - val_loss: 0.8453\n",
            "initial learning rate: 0.0001, current learning rate: 7.290000000000001e-05\n",
            "Epoch 39/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.0876\n",
            "Epoch 00039: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 118s 806ms/step - loss: 0.0877 - val_loss: 0.5106\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 40/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1510\n",
            "Epoch 00040: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 117s 799ms/step - loss: 0.1496 - val_loss: 0.6152\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 41/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1688\n",
            "Epoch 00041: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 116s 796ms/step - loss: 0.1675 - val_loss: 0.4788\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 42/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.0812\n",
            "Epoch 00042: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 116s 793ms/step - loss: 0.0805 - val_loss: 0.7654\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 43/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1172\n",
            "Epoch 00043: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 116s 792ms/step - loss: 0.1169 - val_loss: 0.5710\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 44/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.0655\n",
            "Epoch 00044: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 116s 791ms/step - loss: 0.0650 - val_loss: 0.4628\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 45/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.0406\n",
            "Epoch 00045: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 115s 789ms/step - loss: 0.0403 - val_loss: 0.4913\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 46/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1178\n",
            "Epoch 00046: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 115s 785ms/step - loss: 0.1168 - val_loss: 0.6994\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 47/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.0727\n",
            "Epoch 00047: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 114s 783ms/step - loss: 0.0721 - val_loss: 0.7478\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 48/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1447\n",
            "Epoch 00048: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 114s 780ms/step - loss: 0.1435 - val_loss: 0.5945\n",
            "initial learning rate: 0.0001, current learning rate: 6.561e-05\n",
            "Epoch 49/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1512\n",
            "Epoch 00049: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 114s 781ms/step - loss: 0.1497 - val_loss: 0.6324\n",
            "initial learning rate: 0.0001, current learning rate: 5.904900000000001e-05\n",
            "Epoch 50/50\n",
            "145/146 [============================>.] - ETA: 0s - loss: 0.1620\n",
            "Epoch 00050: val_loss did not improve from 0.45075\n",
            "146/146 [==============================] - 113s 776ms/step - loss: 0.1636 - val_loss: 0.4961\n",
            "Validating ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-59b2ce50d424>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_loss_curve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_cross_entropy_loss\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dense_1_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dense_1_loss'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gackFbY0uqdn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9b034a85-d5a0-4a90-a65e-4be38dab2703"
      },
      "source": [
        "history"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50v2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 128, 128, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 134, 134, 3)  0           input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 64, 64, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 66, 66, 64)   0           conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 32, 32, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_bn (BatchNo (None, 32, 32, 64)   256         pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_preact_relu (Activ (None, 32, 32, 64)   0           conv2_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 32, 32, 64)   4096        conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 32, 32, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_pad (ZeroPadding (None, 34, 34, 64)   0           conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 32, 32, 64)   36864       conv2_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 32, 32, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Add)          (None, 32, 32, 256)  0           conv2_block1_0_conv[0][0]        \n",
            "                                                                 conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_bn (BatchNo (None, 32, 32, 256)  1024        conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_preact_relu (Activ (None, 32, 32, 256)  0           conv2_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 32, 32, 64)   16384       conv2_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 32, 32, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_pad (ZeroPadding (None, 34, 34, 64)   0           conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 32, 32, 64)   36864       conv2_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 32, 32, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 32, 32, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Add)          (None, 32, 32, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_bn (BatchNo (None, 32, 32, 256)  1024        conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_preact_relu (Activ (None, 32, 32, 256)  0           conv2_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 32, 32, 64)   16384       conv2_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 32, 32, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 32, 32, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_pad (ZeroPadding (None, 34, 34, 64)   0           conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 16, 16, 64)   36864       conv2_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 16, 16, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 16, 16, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2D)  (None, 16, 16, 256)  0           conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 16, 16, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Add)          (None, 16, 16, 256)  0           max_pooling2d_9[0][0]            \n",
            "                                                                 conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_bn (BatchNo (None, 16, 16, 256)  1024        conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_preact_relu (Activ (None, 16, 16, 256)  0           conv3_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 16, 16, 128)  32768       conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 16, 16, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_pad (ZeroPadding (None, 18, 18, 128)  0           conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 16, 16, 128)  147456      conv3_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 16, 16, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 16, 16, 512)  131584      conv3_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Add)          (None, 16, 16, 512)  0           conv3_block1_0_conv[0][0]        \n",
            "                                                                 conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_bn (BatchNo (None, 16, 16, 512)  2048        conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_preact_relu (Activ (None, 16, 16, 512)  0           conv3_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 16, 16, 128)  65536       conv3_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 16, 16, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_pad (ZeroPadding (None, 18, 18, 128)  0           conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 16, 16, 128)  147456      conv3_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 16, 16, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Add)          (None, 16, 16, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_bn (BatchNo (None, 16, 16, 512)  2048        conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_preact_relu (Activ (None, 16, 16, 512)  0           conv3_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 16, 16, 128)  65536       conv3_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 16, 16, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_pad (ZeroPadding (None, 18, 18, 128)  0           conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 16, 16, 128)  147456      conv3_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 16, 16, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 16, 16, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Add)          (None, 16, 16, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_bn (BatchNo (None, 16, 16, 512)  2048        conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_preact_relu (Activ (None, 16, 16, 512)  0           conv3_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 16, 16, 128)  65536       conv3_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 16, 16, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 16, 16, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_pad (ZeroPadding (None, 18, 18, 128)  0           conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147456      conv3_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling2D) (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Add)          (None, 8, 8, 512)    0           max_pooling2d_10[0][0]           \n",
            "                                                                 conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_bn (BatchNo (None, 8, 8, 512)    2048        conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_preact_relu (Activ (None, 8, 8, 512)    0           conv4_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 8, 8, 256)    131072      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 8, 8, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_pad (ZeroPadding (None, 10, 10, 256)  0           conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 8, 8, 256)    589824      conv4_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 8, 8, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 8, 8, 1024)   525312      conv4_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Add)          (None, 8, 8, 1024)   0           conv4_block1_0_conv[0][0]        \n",
            "                                                                 conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_bn (BatchNo (None, 8, 8, 1024)   4096        conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_preact_relu (Activ (None, 8, 8, 1024)   0           conv4_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 8, 8, 256)    262144      conv4_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 8, 8, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_pad (ZeroPadding (None, 10, 10, 256)  0           conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 8, 8, 256)    589824      conv4_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 8, 8, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Add)          (None, 8, 8, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_bn (BatchNo (None, 8, 8, 1024)   4096        conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_preact_relu (Activ (None, 8, 8, 1024)   0           conv4_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 8, 8, 256)    262144      conv4_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 8, 8, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_pad (ZeroPadding (None, 10, 10, 256)  0           conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 8, 8, 256)    589824      conv4_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 8, 8, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Add)          (None, 8, 8, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_bn (BatchNo (None, 8, 8, 1024)   4096        conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_preact_relu (Activ (None, 8, 8, 1024)   0           conv4_block4_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 8, 8, 256)    262144      conv4_block4_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 8, 8, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_pad (ZeroPadding (None, 10, 10, 256)  0           conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 8, 8, 256)    589824      conv4_block4_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 8, 8, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Add)          (None, 8, 8, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_bn (BatchNo (None, 8, 8, 1024)   4096        conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_preact_relu (Activ (None, 8, 8, 1024)   0           conv4_block5_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 8, 8, 256)    262144      conv4_block5_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 8, 8, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_pad (ZeroPadding (None, 10, 10, 256)  0           conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 8, 8, 256)    589824      conv4_block5_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 8, 8, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 8, 8, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Add)          (None, 8, 8, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_bn (BatchNo (None, 8, 8, 1024)   4096        conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_preact_relu (Activ (None, 8, 8, 1024)   0           conv4_block6_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 8, 8, 256)    262144      conv4_block6_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 8, 8, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 8, 8, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_pad (ZeroPadding (None, 10, 10, 256)  0           conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    589824      conv4_block6_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling2D) (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Add)          (None, 4, 4, 1024)   0           max_pooling2d_11[0][0]           \n",
            "                                                                 conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_bn (BatchNo (None, 4, 4, 1024)   4096        conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_preact_relu (Activ (None, 4, 4, 1024)   0           conv5_block1_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 4, 4, 512)    524288      conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 4, 4, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_pad (ZeroPadding (None, 6, 6, 512)    0           conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 4, 4, 512)    2359296     conv5_block1_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 4, 4, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 4, 4, 2048)   2099200     conv5_block1_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Add)          (None, 4, 4, 2048)   0           conv5_block1_0_conv[0][0]        \n",
            "                                                                 conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_bn (BatchNo (None, 4, 4, 2048)   8192        conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_preact_relu (Activ (None, 4, 4, 2048)   0           conv5_block2_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 4, 4, 512)    1048576     conv5_block2_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 4, 4, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_pad (ZeroPadding (None, 6, 6, 512)    0           conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 4, 4, 512)    2359296     conv5_block2_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 4, 4, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Add)          (None, 4, 4, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_bn (BatchNo (None, 4, 4, 2048)   8192        conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_preact_relu (Activ (None, 4, 4, 2048)   0           conv5_block3_preact_bn[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 4, 4, 512)    1048576     conv5_block3_preact_relu[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 4, 4, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_pad (ZeroPadding (None, 6, 6, 512)    0           conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 4, 4, 512)    2359296     conv5_block3_2_pad[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 4, 4, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 4, 4, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 4, 4, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Add)          (None, 4, 4, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "post_bn (BatchNormalization)    (None, 4, 4, 2048)   8192        conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "post_relu (Activation)          (None, 4, 4, 2048)   0           post_bn[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,564,800\n",
            "Trainable params: 1,054,720\n",
            "Non-trainable params: 22,510,080\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smS6X7azutr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9cf2bed0-d420-4ed3-ee1c-b692029b0cf1"
      },
      "source": [
        "im"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'drive/My Drive/face_4320.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}